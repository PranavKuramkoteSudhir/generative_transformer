{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding for dummy data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.data_prep.transcode import Tokenizer #get simple encoder for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=\"She sells seashells by the seashore, The shells she sells are surely seashells, So if she sells shells on the seashore, I'm sure she sells seashore shells.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 14, 12, 8, 19, 13, 1, 6, 16, 15, 14, 7, 18, 12, 1, 5, 9, 15, 14, 16, 11, 19, 13, 1, 3, 0, 10, 17, 15, 14, 13, 16, 2]\n"
     ]
    }
   ],
   "source": [
    "# encode the text\n",
    "tk=Tokenizer()\n",
    "enc,dec=tk.transcode(s)\n",
    "encoded_text=tk.encode(s,enc)\n",
    "print(encoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4, 14, 12,  8, 19, 13,  1,  6, 16, 15, 14,  7, 18, 12,  1,  5,  9, 15,\n",
      "        14, 16, 11, 19, 13,  1,  3,  0, 10, 17, 15, 14, 13, 16,  2])\n",
      "length of tt: 33\n"
     ]
    }
   ],
   "source": [
    "# convert text into torch tensor\n",
    "tt=torch.tensor(encoded_text)\n",
    "print(tt)\n",
    "print('length of tt:',len(tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0166, -0.4668,  2.0909,  0.6149, -0.5621],\n",
      "        [-0.4143, -1.3001, -0.1012, -1.0999, -0.4687],\n",
      "        [-0.8400,  0.6868, -0.0678, -0.0886, -0.3124],\n",
      "        [-0.3552,  0.7142,  0.4198, -1.7672, -0.4996],\n",
      "        [ 0.3083, -0.2947, -0.7662, -0.9962,  0.7781],\n",
      "        [-0.6856, -0.7326,  0.3004, -0.3630,  1.5822],\n",
      "        [-0.4430,  1.8462, -0.7115,  1.0425, -0.7618],\n",
      "        [-0.4830, -0.7771, -0.4484, -1.1668,  0.5006],\n",
      "        [ 1.3251, -1.1627, -2.1186,  0.5470, -1.1668],\n",
      "        [-1.0199,  1.6283,  0.3475, -0.1191, -1.5415],\n",
      "        [ 0.6421,  1.5016, -0.1955,  0.7438,  1.3611],\n",
      "        [ 0.7930,  0.4777,  0.9248,  0.8841,  0.7809],\n",
      "        [ 0.4664, -2.2619,  0.5374, -0.8002,  1.6715],\n",
      "        [-0.3596, -2.1513, -1.3592, -0.7203, -0.5679],\n",
      "        [-0.5438,  1.8492, -0.0085, -0.4166,  0.4745],\n",
      "        [-0.4395, -0.7560,  1.1363,  1.2756,  0.1642],\n",
      "        [ 2.7160,  0.8090, -1.0697, -0.1990, -0.9886],\n",
      "        [-2.1535,  0.3912,  1.5218, -0.5329,  0.1319],\n",
      "        [-0.2966, -0.3325, -0.4449, -0.1949, -0.0895],\n",
      "        [-0.4446, -1.2036, -0.8498, -0.1835, -0.7464],\n",
      "        [ 0.7142, -0.6138,  0.8019,  1.8988, -0.2278],\n",
      "        [-1.3464,  0.9176,  0.4063,  0.5828, -1.8027],\n",
      "        [ 2.2014, -0.1872,  0.0557,  0.2540,  1.0183],\n",
      "        [-0.0824, -0.0791, -0.2089, -0.3442,  1.8142],\n",
      "        [-1.2751, -1.6557, -0.0941, -1.3915, -0.3012],\n",
      "        [ 0.3881,  0.6736, -2.2534, -2.0612,  0.5554],\n",
      "        [-0.5718, -0.2844, -2.7623,  0.8921,  0.3671],\n",
      "        [ 0.5021,  0.9858, -0.2068,  1.4470, -0.9580],\n",
      "        [-0.9873, -0.6070, -0.9646,  0.9868, -0.9512],\n",
      "        [-0.7114,  0.5763, -1.8603, -1.1927,  0.3978],\n",
      "        [ 0.4384, -1.9944, -0.4657,  0.8998, -0.7435],\n",
      "        [ 0.7210,  2.6888,  0.1458, -0.7261,  1.8866],\n",
      "        [ 0.1422,  0.2733, -0.4433, -0.5858, -0.5670],\n",
      "        [ 1.2877,  0.2881, -0.0710,  0.6117,  0.5042],\n",
      "        [-0.3404, -1.3312,  1.3698,  0.3584, -0.5757],\n",
      "        [-0.2979, -0.2908,  0.7011, -0.9563, -0.6499],\n",
      "        [-0.3879, -1.2187, -0.1161,  0.0893,  0.7953],\n",
      "        [ 0.5151,  0.6487,  0.4946,  1.6608, -0.5382],\n",
      "        [ 0.6616, -0.4102, -1.7070, -0.9099, -0.8141],\n",
      "        [ 0.2601, -1.0871, -0.3894,  0.3904, -0.0590],\n",
      "        [ 0.4584,  0.3003,  0.1655, -0.0631,  0.3966],\n",
      "        [-0.2473, -2.3111, -1.6667,  0.9899, -0.5587],\n",
      "        [-0.2832,  0.9303, -0.2022, -0.9712, -0.5662],\n",
      "        [ 0.4277,  0.7050,  2.0228, -0.5358,  1.1040],\n",
      "        [ 0.1907,  1.5624,  2.0022, -0.9607,  0.0759],\n",
      "        [ 0.9433, -0.2076,  0.6664,  1.9460, -0.0922],\n",
      "        [ 0.2744, -1.7344, -0.5801,  1.1602,  1.2013],\n",
      "        [-0.4110, -0.5037,  0.7305,  0.9236,  0.2028],\n",
      "        [ 0.6092,  0.3379, -0.7356,  1.5769,  0.1530],\n",
      "        [-0.6477, -0.5846,  0.7432, -2.0749,  0.1314]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create an embedding layer with desired vocabulary size and number of dimensions required for each word in vocab\n",
    "vocab_size=50\n",
    "dimensions=5\n",
    "torch.manual_seed(43)\n",
    "embed=torch.nn.Embedding(vocab_size,dimensions)\n",
    "print(embed.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3083, -0.2947, -0.7662, -0.9962,  0.7781],\n",
      "        [-0.5438,  1.8492, -0.0085, -0.4166,  0.4745],\n",
      "        [ 0.4664, -2.2619,  0.5374, -0.8002,  1.6715],\n",
      "        [ 1.3251, -1.1627, -2.1186,  0.5470, -1.1668],\n",
      "        [-0.4446, -1.2036, -0.8498, -0.1835, -0.7464],\n",
      "        [-0.3596, -2.1513, -1.3592, -0.7203, -0.5679],\n",
      "        [-0.4143, -1.3001, -0.1012, -1.0999, -0.4687],\n",
      "        [-0.4430,  1.8462, -0.7115,  1.0425, -0.7618],\n",
      "        [ 2.7160,  0.8090, -1.0697, -0.1990, -0.9886],\n",
      "        [-0.4395, -0.7560,  1.1363,  1.2756,  0.1642],\n",
      "        [-0.5438,  1.8492, -0.0085, -0.4166,  0.4745],\n",
      "        [-0.4830, -0.7771, -0.4484, -1.1668,  0.5006],\n",
      "        [-0.2966, -0.3325, -0.4449, -0.1949, -0.0895],\n",
      "        [ 0.4664, -2.2619,  0.5374, -0.8002,  1.6715],\n",
      "        [-0.4143, -1.3001, -0.1012, -1.0999, -0.4687],\n",
      "        [-0.6856, -0.7326,  0.3004, -0.3630,  1.5822],\n",
      "        [-1.0199,  1.6283,  0.3475, -0.1191, -1.5415],\n",
      "        [-0.4395, -0.7560,  1.1363,  1.2756,  0.1642],\n",
      "        [-0.5438,  1.8492, -0.0085, -0.4166,  0.4745],\n",
      "        [ 2.7160,  0.8090, -1.0697, -0.1990, -0.9886],\n",
      "        [ 0.7930,  0.4777,  0.9248,  0.8841,  0.7809],\n",
      "        [-0.4446, -1.2036, -0.8498, -0.1835, -0.7464],\n",
      "        [-0.3596, -2.1513, -1.3592, -0.7203, -0.5679],\n",
      "        [-0.4143, -1.3001, -0.1012, -1.0999, -0.4687],\n",
      "        [-0.3552,  0.7142,  0.4198, -1.7672, -0.4996],\n",
      "        [-0.0166, -0.4668,  2.0909,  0.6149, -0.5621],\n",
      "        [ 0.6421,  1.5016, -0.1955,  0.7438,  1.3611],\n",
      "        [-2.1535,  0.3912,  1.5218, -0.5329,  0.1319],\n",
      "        [-0.4395, -0.7560,  1.1363,  1.2756,  0.1642],\n",
      "        [-0.5438,  1.8492, -0.0085, -0.4166,  0.4745],\n",
      "        [-0.3596, -2.1513, -1.3592, -0.7203, -0.5679],\n",
      "        [ 2.7160,  0.8090, -1.0697, -0.1990, -0.9886],\n",
      "        [-0.8400,  0.6868, -0.0678, -0.0886, -0.3124]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#create embeded text map by mapping encoded tensor to the created embeded layer\n",
    "#The current embedding layer only contains random weights but this can be trained with the actual text to assign text semantic meaning\n",
    "embeded_text=(embed(tt))\n",
    "print(embeded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
