{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "\n",
    "from src.logger import logging\n",
    "from src.exceptions import CustomException\n",
    "\n",
    "from src.GPT.model_forward import GPTModel\n",
    "from src.data_prep.tokenization import CustomDataset\n",
    "from src.data_prep.data_load import CustomDataLoad\n",
    "from src.utils import get_text\n",
    "from src.generate import generate_text,token_to_text,text_to_token_ids\n",
    "from src.mainloop.forward_pass import training_loop,calc_loss_loader,calc_loss,model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x10a1cac00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/pranavks/Desktop/Projects/transformer/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/pranavks/Desktop/Projects/transformer/venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1582, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/pranavks/miniconda3/lib/python3.12/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pranavks/miniconda3/lib/python3.12/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pranavks/miniconda3/lib/python3.12/multiprocessing/connection.py\", line 1136, in wait\n",
      "    ready = selector.select(timeout)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pranavks/miniconda3/lib/python3.12/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/pranavks/Desktop/Projects/transformer/venv/lib/python3.12/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 13904) is killed by signal: Abort trap: 6. \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    data=get_text(\"/Users/pranavks/Desktop/Projects/transformer/artifacts\")\n",
    "    train=data[int(len(data)*0.8):]\n",
    "    test=data[:-int(len(data)*0.2)]\n",
    "    load_dotenv()\n",
    "    \n",
    "    dl_train=CustomDataLoad(\n",
    "        train,\n",
    "        int(os.getenv('context_len')),\n",
    "        int(os.getenv('batch_size')),\n",
    "        int(os.getenv('stride')),\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        workers=4)\n",
    "    dl_test=CustomDataLoad(\n",
    "        test,\n",
    "        int(os.getenv('context_len')),\n",
    "        int(os.getenv('batch_size')),\n",
    "        int(os.getenv('stride')),\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        workers=4)\n",
    "    data_load_train=dl_train.get_data_loader()\n",
    "    data_load_test=dl_test.get_data_loader()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model=GPTModel()\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer=torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.01)\n",
    "    epochs=5\n",
    "\n",
    "    training_loop(model,data_load_train,data_load_test,optimizer,device,epochs,5,evaluation_iteration=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "starter=\"Diomedes gave me a quizzical glance. I hesitated. \\“If I’m going to get anywhere with her, I need Alicia to be able to think, and feel.\\” \\“Absolutely. And your concern is…?\\” \\“It’s impossible to get through to someone when they’re so heavily medicated. It’s like she’s six feet underwater.\\” Diomedes frowned. \\“I wouldn’t go that far.\"\n",
    "generated_tokens=generate_text(model,text_to_token_ids(text=starter),100,int(os.getenv('context_len')),0.0,20,None)\n",
    "generated_text=token_to_text(token_ids=generated_tokens)\n",
    "logging.info(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
